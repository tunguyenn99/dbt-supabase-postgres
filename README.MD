# ğŸ› ï¸ DBT Retail PG â€“ Postgres + Supabase + Airflow Project

This repository provides a complete setup for working with **DBT (Data Build Tool)** using a **Postgres** database hosted on **Supabase**, with automation handled by **Apache Airflow** and **Cosmos**.

---

## ğŸ“ Project Structure

```
dbt-supabase-postgres/
â”œâ”€â”€ airflow_retail_pg/        # Airflow project (DAGs, configs)
â”œâ”€â”€ logs/                     # Logs directory (Airflow runtime)
â”œâ”€â”€ retail_pg/                # DBT project folder
â”œâ”€â”€ venv/                     # Python virtual environment (excluded by .gitignore)
â”œâ”€â”€ .env                      # Environment variables (not committed)
â”œâ”€â”€ .env-sample               # Sample .env file (edit and rename to .env)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt          # Python dependencies
```

---

## âš™ï¸ Setup Instructions (Ubuntu)

1. **Clone the repository:**

```bash
git clone https://github.com/tunguyenn99/dbt-supabase-postgres.git
cd dbt-supabase-postgres
```

2. **Create Python virtual environment:**

```bash
python3 -m venv venv
source venv/bin/activate
```

3. **Install dependencies:**

```bash
pip install -r requirements.txt
```

4. **Prepare environment variables:**

- Copy the sample file and update credentials:

```bash
cp .env-sample .env
# Then edit .env with your Supabase/Postgres credentials
```

5. **Navigate to the DBT project folder:**

```bash
cd retail_pg
```

After that, create the following folders if not already available:

```
retail_pg/
â”‚
â”œâ”€â”€ models/                     # SQL models for data transformation
â”‚   â”œâ”€â”€ staging/                # Normalize raw source data
â”‚   â”œâ”€â”€ intermediate/           # Join and transform staging models
â”‚   â”œâ”€â”€ marts/                  # Final models for reporting
â”‚   â””â”€â”€ schema.yml              # Metadata and tests
â”‚
â”œâ”€â”€ seeds/                      # Sample/reference data
â”‚   â”œâ”€â”€ csv_files/              # Source CSV files (not committed)
â”œâ”€â”€ macros/                     # Custom reusable SQL functions
â”œâ”€â”€ snapshots/                  # Table change tracking
â”œâ”€â”€ analyses/                   # Temporary analysis queries
â”œâ”€â”€ tests/                      # Custom tests
â”œâ”€â”€ dbt_project.yml             # Main DBT config
â”œâ”€â”€ profiles.yml                # DB connection config
â””â”€â”€ README.md                   # Project guide
```

6. **Run DBT commands:**

- Debug connection:
  ```bash
  dbt debug
  ```
- Run models:
  ```bash
  dbt run
  ```
- Test models:
  ```bash
  dbt test
  ```
- Generate docs:
  ```bash
  dbt docs generate
  dbt docs serve --port 8081
  ```

Below is a successful generation of `dbt docs`:

![DBT Docs Result](./images/dbt_docs.png)


---

## ğŸ”„ Automating with Airflow + Cosmos

Thanks to detailed instructions in below references sections to set up Airflow + Cosmos, I can configure Cosmos DAGs, and run scheduled DBT pipelines.

### âœ… Sample DAG Execution Result

Below is a successful run of the DBT pipeline DAG:

![DAG Execution Result](./images/dags_result.png)

---

## ğŸ“š References
- [DBT Documentation](https://docs.getdbt.com/)
- [Supabase Docs](https://supabase.com/docs)
- [Apache Airflow Docs](https://airflow.apache.org/docs/)
- [Astronomer Cosmos](https://astronomer.io/docs/cosmos)
